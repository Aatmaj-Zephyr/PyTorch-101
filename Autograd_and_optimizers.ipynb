{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd and optimizers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPfB5886usTsiPrh6BGTrq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smlra-kjsce/Pytorch-101/blob/main/Autograd_and_optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqnglkFJ8Cj0"
      },
      "source": [
        "# Autograd\r\n",
        "\r\n",
        "Autograd is used to calulate the gradients of a tensor. It is a very useful tool when we know that we would require to calulcate the gradients of a tensor. It stores a computational graph of the tensor. It simply works by setting requires_grad=True\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAWct_lLSvm6",
        "outputId": "33df8635-23c1-4153-f2be-621b127cb262"
      },
      "source": [
        "x=torch.randn(3,2,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "# In the output, we can see the AddBackward, i.e. the output has been obtained by adding\r\n",
        "y = x + 10\r\n",
        "print(y)\r\n",
        "\r\n",
        "# In the output, we can see the MulBackward, i.e. the output has been obtained by mutiplication\r\n",
        "z = y*y*y\r\n",
        "print(z)\r\n",
        "\r\n",
        "# In the output, we can see the MeanBackward, i.e. the output has been obtained by taking average\r\n",
        "w = z.mean()\r\n",
        "print(w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.5518,  0.7587],\n",
            "        [ 1.2387,  0.4535],\n",
            "        [-0.0475, -0.1571]], requires_grad=True)\n",
            "tensor([[10.5518, 10.7587],\n",
            "        [11.2387, 10.4535],\n",
            "        [ 9.9525,  9.8429]], grad_fn=<AddBackward0>)\n",
            "tensor([[1174.8466, 1245.3203],\n",
            "        [1419.5282, 1142.3099],\n",
            "        [ 985.8307,  953.6094]], grad_fn=<MulBackward0>)\n",
            "tensor(1153.5742, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFq3ZEw5T4H2"
      },
      "source": [
        "To see the gradients of the tensor, we just need to call .backward() method. The gradients are calculated with respect to the original tensor and are stored in original_tensor.grads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPo2BBITT0hU",
        "outputId": "1440f4c2-5331-4560-a1a7-55d4108721a1"
      },
      "source": [
        "x=torch.randn(2,4,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "y=x+2\r\n",
        "print(y)\r\n",
        "\r\n",
        "z=y.mean()\r\n",
        "print(z)\r\n",
        "\r\n",
        "z.backward()\r\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0426, -1.5887, -1.3168, -1.5934],\n",
            "        [ 0.7231, -0.5572, -0.3969,  0.3268]], requires_grad=True)\n",
            "tensor([[0.9574, 0.4113, 0.6832, 0.4066],\n",
            "        [2.7231, 1.4428, 1.6031, 2.3268]], grad_fn=<AddBackward0>)\n",
            "tensor(1.3193, grad_fn=<MeanBackward0>)\n",
            "tensor([[0.1250, 0.1250, 0.1250, 0.1250],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnO4CD_TWPhE"
      },
      "source": [
        "Note that the last value was a single valued tensor i.e. a sclar(as we computed mean). So there was no need of specifying the vector with respect to whose gradient we needed to calculate. However if the last value would have been a vector, we need to pass a vector of the same dimension as that of the last value to the .grad() function in order for pytorch to know in respect to which values of the vector, it needs to calculate the gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWITyeo2TFCh",
        "outputId": "87a8aa3c-ce43-4d28-dc6f-6c54ad9f5f64"
      },
      "source": [
        "x=torch.randn(2,4,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "y=x+2\r\n",
        "print(y)\r\n",
        "\r\n",
        "z=y*y\r\n",
        "print(z)\r\n",
        "\r\n",
        "w = torch.randn(2,4)\r\n",
        "z.backward(w)\r\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.8952,  0.9157, -0.4119,  1.4888],\n",
            "        [-0.1434, -0.7998,  1.0399, -2.0248]], requires_grad=True)\n",
            "tensor([[ 1.1048,  2.9157,  1.5881,  3.4888],\n",
            "        [ 1.8566,  1.2002,  3.0399, -0.0248]], grad_fn=<AddBackward0>)\n",
            "tensor([[1.2206e+00, 8.5011e+00, 2.5219e+00, 1.2172e+01],\n",
            "        [3.4469e+00, 1.4405e+00, 9.2412e+00, 6.1321e-04]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "tensor([[-3.6753, -3.5920,  1.4077,  2.7785],\n",
            "        [ 6.5904,  0.6255, -1.8888,  0.0136]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfJlN5tVXhNd"
      },
      "source": [
        "Sometimes, we do not require pytorch to track the graidents, so for such times, we can either directly set the requires_grad to false or use x.detach() or wrap the functions in 'with torch.no_grad():'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVZF7aJhWOU7",
        "outputId": "c1e07b46-6e39-4588-82cd-9a76dd2fe5ca"
      },
      "source": [
        "x=torch.randn(2,4,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "# As we can see, we do not have the grad_fn in the y and z tensors\r\n",
        "y = x.detach()\r\n",
        "print(y)\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  z = x+1\r\n",
        "  print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6527,  0.9514,  0.7132,  0.1135],\n",
            "        [ 1.1598, -1.6017, -0.1744, -0.3231]], requires_grad=True)\n",
            "tensor([[ 0.6527,  0.9514,  0.7132,  0.1135],\n",
            "        [ 1.1598, -1.6017, -0.1744, -0.3231]])\n",
            "tensor([[ 1.6527,  1.9514,  1.7132,  1.1135],\n",
            "        [ 2.1598, -0.6017,  0.8256,  0.6769]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXT42aO0ZLbG"
      },
      "source": [
        "Many a times during training, we need to flush out the gradients, so that they are not accumulated again and again during other epochs. This is achieved simply by using the tensor.grad.zero_() method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb6cW1HfYGte",
        "outputId": "b15377fa-3070-45d6-bff0-e2fa77069110"
      },
      "source": [
        "x=torch.randn(2,4,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "  y = (x+2).mean()\r\n",
        "  y.backward()\r\n",
        "  print(x.grad)\r\n",
        "\r\n",
        "print(\"The above adds the gradients again and again and hence is incorrect. The correct one is shown below \")\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "  y = (x+2).mean()\r\n",
        "  y.backward()\r\n",
        "  print(x.grad)\r\n",
        "  x.grad.zero_()     # This flushes out the gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4291,  1.9847, -0.7129, -0.7841],\n",
            "        [-1.3326, -0.3136, -1.0314,  0.8431]], requires_grad=True)\n",
            "tensor([[0.1250, 0.1250, 0.1250, 0.1250],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
            "tensor([[0.3750, 0.3750, 0.3750, 0.3750],\n",
            "        [0.3750, 0.3750, 0.3750, 0.3750]])\n",
            "The above adds the gradients again and again and hence is incorrect. The correct one is shown below \n",
            "tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000]])\n",
            "tensor([[0.1250, 0.1250, 0.1250, 0.1250],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "tensor([[0.1250, 0.1250, 0.1250, 0.1250],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjgCePcibb-L"
      },
      "source": [
        "Having learnt this, now let us implement a small linear regression in pytorch with the recently learnt autograds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkOdjvidZy-9",
        "outputId": "23796ab5-7073-4f18-ab17-85da6fac68a4"
      },
      "source": [
        "# We import nn for the loss function.\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "# Our data points, here we have only one :( \r\n",
        "x = torch.tensor([10.0,5.0])\r\n",
        "y = torch.tensor([10.0,5.0])\r\n",
        "\r\n",
        "# Initialize w and b as 1 and 0 respectively\r\n",
        "w = torch.ones(1,requires_grad=True)\r\n",
        "b = torch.ones(1,requires_grad=True)\r\n",
        "\r\n",
        "# Define the forward pass\r\n",
        "def forward(x):\r\n",
        "  return w*x+b\r\n",
        "\r\n",
        "# Define hyperparameters i.e. learning rate and epochs\r\n",
        "epochs = 100\r\n",
        "learning_rate = 0.001\r\n",
        "\r\n",
        "# Build the linear regression loop and train it for sepcified epochs. We use simple Mean squared loss (MSE)\r\n",
        "for i in range (epochs):\r\n",
        "\r\n",
        "  # Forward Pass\r\n",
        "  y_predicted = forward(x)\r\n",
        "\r\n",
        "  # caluclating Loss\r\n",
        "  loss = nn.MSELoss()\r\n",
        "  L = loss(y,y_predicted)\r\n",
        "\r\n",
        "  # Calculating gradients for the loss (Backward pass or backpropagation)\r\n",
        "  L.backward()\r\n",
        "\r\n",
        "  # Manually Updating weights starts here\r\n",
        "  # Since we do not need pytorch to track this updates, as this is not used in backprop, we use it inside no_grad()\r\n",
        "  # with torch.no_grad():\r\n",
        "    # w -= learning_rate*(w.grad)\r\n",
        "    # b -= learning_rate*(b.grad)\r\n",
        "\r\n",
        "  # WARNING: Do not forget to flush out weights\r\n",
        "  # w.grad.zero_()\r\n",
        "  # b.grad.zero_()\r\n",
        "  # Manually updating weights end here\r\n",
        "\r\n",
        "  # Instead of manually updating the weights, we can use optimizer present in pytorch.nn module\r\n",
        "  # Here, we have used SGD(Stochastic Gradient Descent)\r\n",
        "  optimizer = torch.optim.SGD([w,b],lr=learning_rate)\r\n",
        "  optimizer.step()\r\n",
        "  optimizer.zero_grad()\r\n",
        "\r\n",
        "  if (i%10 == 0 ):\r\n",
        "    print(\"Epoch \" + str(i))\r\n",
        "    print(\"Loss \" + str(L))\r\n",
        "\r\n",
        "print(y_predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Loss tensor(1., grad_fn=<MseLossBackward>)\n",
            "Epoch 10\n",
            "Loss tensor(0.1568, grad_fn=<MseLossBackward>)\n",
            "Epoch 20\n",
            "Loss tensor(0.1004, grad_fn=<MseLossBackward>)\n",
            "Epoch 30\n",
            "Loss tensor(0.0963, grad_fn=<MseLossBackward>)\n",
            "Epoch 40\n",
            "Loss tensor(0.0957, grad_fn=<MseLossBackward>)\n",
            "Epoch 50\n",
            "Loss tensor(0.0953, grad_fn=<MseLossBackward>)\n",
            "Epoch 60\n",
            "Loss tensor(0.0949, grad_fn=<MseLossBackward>)\n",
            "Epoch 70\n",
            "Loss tensor(0.0945, grad_fn=<MseLossBackward>)\n",
            "Epoch 80\n",
            "Loss tensor(0.0942, grad_fn=<MseLossBackward>)\n",
            "Epoch 90\n",
            "Loss tensor(0.0938, grad_fn=<MseLossBackward>)\n",
            "tensor([9.8048, 5.3858], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyM2NdKQS-AV"
      },
      "source": [
        "A very important point to note in the above code is that while updating the w and b if we write w = w - lr * (w.grad), it won't work. This is becasue this statement will store it in a new 'w'. Hence we use w -= lr * (w.grad)"
      ]
    }
  ]
}