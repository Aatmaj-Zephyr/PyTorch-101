{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd and optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+EOh04suH6VK1rGTV0B4D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smlra-kjsce/PyTorch-101/blob/main/Autograd_and_optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqtBuCpHp0gY"
      },
      "source": [
        "# Forward & Backward Passes \r\n",
        "\r\n",
        "The forward pass of any neural network or for that matter a linear layer is just computing the predicted values of y from the given x value and the current weights.\r\n",
        "\r\n",
        "After the forward pass, loss is calculated. The backward pass deals with computing the gradients, i.e. the derivatives of the loss wrt the weights from the loss so that the weights can be updated for further iterations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duviVH7gqLLw"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpcOesiYncDY"
      },
      "source": [
        "# Gradient Descent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "QbGbbXfcoOeR",
        "outputId": "0bc96d2b-e357-4d2b-ea83-2bcab920e128"
      },
      "source": [
        "from IPython.display import Image\r\n",
        "Image(url=\"https://thumbs.gfycat.com/AngryInconsequentialDiplodocus-size_restricted.gif\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://thumbs.gfycat.com/AngryInconsequentialDiplodocus-size_restricted.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8sDDWxwn0Cb"
      },
      "source": [
        "We usually update the weights using gradient descent algorithm. It tries to find the minima of the loss function so that the model is optimized. The formula to update the weights using gradient descent is\r\n",
        "\r\n",
        "w = w - (lr) * (w.gradients)\r\n",
        "\r\n",
        "Gradient descent is one of the many optimizers that can be used. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqnglkFJ8Cj0"
      },
      "source": [
        "# Autograd\r\n",
        "\r\n",
        "Autograd is used to calulate the gradients of a tensor. It is a very useful tool when we know that we would require to calulcate the gradients of a tensor. It stores a computational graph of the tensor. It simply works by setting requires_grad=True\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D4DCRgLLnq6"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "# Forward, backwar, gradient descent explanation to be added"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAWct_lLSvm6",
        "outputId": "7d5f7fc8-6a3f-486e-c95b-1a8f136cf5bb"
      },
      "source": [
        "x=torch.randn(3,2,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "# In the output, we can see the AddBackward, i.e. the output has been obtained by adding\r\n",
        "y = x + 10\r\n",
        "print(y)\r\n",
        "\r\n",
        "# In the output, we can see the MulBackward, i.e. the output has been obtained by mutiplication\r\n",
        "z = y*y*y\r\n",
        "print(z)\r\n",
        "\r\n",
        "# In the output, we can see the MeanBackward, i.e. the output has been obtained by taking average\r\n",
        "w = z.mean()\r\n",
        "print(w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2072, -0.4379],\n",
            "        [-0.2602,  1.0029],\n",
            "        [ 0.1143, -1.3281]], requires_grad=True)\n",
            "tensor([[ 9.7928,  9.5621],\n",
            "        [ 9.7398, 11.0029],\n",
            "        [10.1143,  8.6719]], grad_fn=<AddBackward0>)\n",
            "tensor([[ 939.1259,  874.3044],\n",
            "        [ 923.9620, 1332.0693],\n",
            "        [1034.6919,  652.1325]], grad_fn=<MulBackward0>)\n",
            "tensor(959.3810, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFq3ZEw5T4H2"
      },
      "source": [
        "To see the gradients of the tensor, we just need to call .backward() method. The gradients are calculated with respect to the original tensor and are stored in original_tensor.grads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPo2BBITT0hU",
        "outputId": "1440f4c2-5331-4560-a1a7-55d4108721a1"
      },
      "source": [
        "x=torch.randn(2,4,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "y=x+2\r\n",
        "print(y)\r\n",
        "\r\n",
        "z=y.mean()\r\n",
        "print(z)\r\n",
        "\r\n",
        "z.backward()\r\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0426, -1.5887, -1.3168, -1.5934],\n",
            "        [ 0.7231, -0.5572, -0.3969,  0.3268]], requires_grad=True)\n",
            "tensor([[0.9574, 0.4113, 0.6832, 0.4066],\n",
            "        [2.7231, 1.4428, 1.6031, 2.3268]], grad_fn=<AddBackward0>)\n",
            "tensor(1.3193, grad_fn=<MeanBackward0>)\n",
            "tensor([[0.1250, 0.1250, 0.1250, 0.1250],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnO4CD_TWPhE"
      },
      "source": [
        "Note that the last value was a single valued tensor i.e. a sclar(as we computed mean). So there was no need of specifying the vector with respect to whose gradient we needed to calculate. However if the last value would have been a vector, we need to pass a vector of the same dimension as that of the last value to the .grad() function in order for pytorch to know in respect to which values of the vector, it needs to calculate the gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWITyeo2TFCh",
        "outputId": "87a8aa3c-ce43-4d28-dc6f-6c54ad9f5f64"
      },
      "source": [
        "x=torch.randn(2,4,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "y=x+2\r\n",
        "print(y)\r\n",
        "\r\n",
        "z=y*y\r\n",
        "print(z)\r\n",
        "\r\n",
        "w = torch.randn(2,4)\r\n",
        "z.backward(w)\r\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.8952,  0.9157, -0.4119,  1.4888],\n",
            "        [-0.1434, -0.7998,  1.0399, -2.0248]], requires_grad=True)\n",
            "tensor([[ 1.1048,  2.9157,  1.5881,  3.4888],\n",
            "        [ 1.8566,  1.2002,  3.0399, -0.0248]], grad_fn=<AddBackward0>)\n",
            "tensor([[1.2206e+00, 8.5011e+00, 2.5219e+00, 1.2172e+01],\n",
            "        [3.4469e+00, 1.4405e+00, 9.2412e+00, 6.1321e-04]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "tensor([[-3.6753, -3.5920,  1.4077,  2.7785],\n",
            "        [ 6.5904,  0.6255, -1.8888,  0.0136]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfJlN5tVXhNd"
      },
      "source": [
        "Sometimes, we do not require pytorch to track the graidents, so for such times, we can either directly set the requires_grad to false or use x.detach() or wrap the functions in 'with torch.no_grad():'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVZF7aJhWOU7",
        "outputId": "c1e07b46-6e39-4588-82cd-9a76dd2fe5ca"
      },
      "source": [
        "x=torch.randn(2,4,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "# As we can see, we do not have the grad_fn in the y and z tensors\r\n",
        "y = x.detach()\r\n",
        "print(y)\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  z = x+1\r\n",
        "  print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6527,  0.9514,  0.7132,  0.1135],\n",
            "        [ 1.1598, -1.6017, -0.1744, -0.3231]], requires_grad=True)\n",
            "tensor([[ 0.6527,  0.9514,  0.7132,  0.1135],\n",
            "        [ 1.1598, -1.6017, -0.1744, -0.3231]])\n",
            "tensor([[ 1.6527,  1.9514,  1.7132,  1.1135],\n",
            "        [ 2.1598, -0.6017,  0.8256,  0.6769]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXT42aO0ZLbG"
      },
      "source": [
        "Many a times during training, we need to flush out the gradients, so that they are not accumulated again and again during other epochs. This is achieved simply by using the tensor.grad.zero_() method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb6cW1HfYGte",
        "outputId": "b15377fa-3070-45d6-bff0-e2fa77069110"
      },
      "source": [
        "x=torch.randn(2,4,requires_grad=True)\r\n",
        "print(x)\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "  y = (x+2).mean()\r\n",
        "  y.backward()\r\n",
        "  print(x.grad)\r\n",
        "\r\n",
        "print(\"The above adds the gradients again and again and hence is incorrect. The correct one is shown below \")\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "  y = (x+2).mean()\r\n",
        "  y.backward()\r\n",
        "  print(x.grad)\r\n",
        "  x.grad.zero_()     # This flushes out the gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4291,  1.9847, -0.7129, -0.7841],\n",
            "        [-1.3326, -0.3136, -1.0314,  0.8431]], requires_grad=True)\n",
            "tensor([[0.1250, 0.1250, 0.1250, 0.1250],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
            "tensor([[0.3750, 0.3750, 0.3750, 0.3750],\n",
            "        [0.3750, 0.3750, 0.3750, 0.3750]])\n",
            "The above adds the gradients again and again and hence is incorrect. The correct one is shown below \n",
            "tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000]])\n",
            "tensor([[0.1250, 0.1250, 0.1250, 0.1250],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "tensor([[0.1250, 0.1250, 0.1250, 0.1250],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjgCePcibb-L"
      },
      "source": [
        "Having learnt this, now let us implement a small linear regression in pytorch with the recently learnt autograds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiIPoaaYh2Wg"
      },
      "source": [
        "Here's an overview of the linear regression algorithm\r\n",
        "1. We assume the hypothesis function as y_pred = w * x + c\r\n",
        "\r\n",
        "2. Loss is calculated using y_predicted and y_true. \r\n",
        "\r\n",
        "3. The derivative of loss wrt the weights is computed. \r\n",
        "\r\n",
        "4. Weights are updated using gradient descent and we again go to step one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkOdjvidZy-9",
        "outputId": "62f8eda9-4672-4a12-8816-33219fb304b4"
      },
      "source": [
        "# We import nn for the loss function.\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "# Our data points, here we have only two :( \r\n",
        "x = torch.tensor([10.0,5.0])\r\n",
        "y = torch.tensor([10.0,5.0])\r\n",
        "\r\n",
        "# Initialize w and b as 1 and 0 respectively\r\n",
        "w = torch.ones(1,requires_grad=True)\r\n",
        "b = torch.ones(1,requires_grad=True)\r\n",
        "\r\n",
        "# Define the forward pass\r\n",
        "def forward(x):\r\n",
        "  return w*x+b\r\n",
        "\r\n",
        "# Define hyperparameters i.e. learning rate and epochs\r\n",
        "epochs = 100\r\n",
        "learning_rate = 0.001\r\n",
        "\r\n",
        "# Build the linear regression loop and train it for sepcified epochs. We use simple Mean squared loss (MSE)\r\n",
        "for i in range (epochs+1):\r\n",
        "\r\n",
        "  # Forward Pass\r\n",
        "  y_predicted = forward(x)\r\n",
        "\r\n",
        "  # caluclating Loss\r\n",
        "  loss = nn.MSELoss()\r\n",
        "  L = loss(y,y_predicted)\r\n",
        "\r\n",
        "  # Calculating gradients for the loss (Backward pass or backpropagation)\r\n",
        "  L.backward()\r\n",
        "\r\n",
        "  # Manually Updating weights starts here\r\n",
        "  # Since we do not need pytorch to track this updates, as this is not used in backprop, we use it inside no_grad()\r\n",
        "  # with torch.no_grad():\r\n",
        "    # w -= learning_rate*(w.grad)\r\n",
        "    # b -= learning_rate*(b.grad)\r\n",
        "\r\n",
        "  # WARNING: Do not forget to flush out weights\r\n",
        "  # w.grad.zero_()\r\n",
        "  # b.grad.zero_()\r\n",
        "  # Manually updating weights end here\r\n",
        "\r\n",
        "  # Instead of manually updating the weights, we can use optimizer present in pytorch.nn module\r\n",
        "  # Here, we have used SGD(Stochastic Gradient Descent)\r\n",
        "  optimizer = torch.optim.SGD([w,b],lr=learning_rate)\r\n",
        "  optimizer.step()\r\n",
        "  optimizer.zero_grad()\r\n",
        "\r\n",
        "  if (i%10 == 0 ):\r\n",
        "    print(\"Epoch \" + str(i))\r\n",
        "    print(\"Loss \" + str(L))\r\n",
        "\r\n",
        "print(y_predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Loss tensor(1., grad_fn=<MseLossBackward>)\n",
            "Epoch 10\n",
            "Loss tensor(0.1568, grad_fn=<MseLossBackward>)\n",
            "Epoch 20\n",
            "Loss tensor(0.1004, grad_fn=<MseLossBackward>)\n",
            "Epoch 30\n",
            "Loss tensor(0.0963, grad_fn=<MseLossBackward>)\n",
            "Epoch 40\n",
            "Loss tensor(0.0957, grad_fn=<MseLossBackward>)\n",
            "Epoch 50\n",
            "Loss tensor(0.0953, grad_fn=<MseLossBackward>)\n",
            "Epoch 60\n",
            "Loss tensor(0.0949, grad_fn=<MseLossBackward>)\n",
            "Epoch 70\n",
            "Loss tensor(0.0945, grad_fn=<MseLossBackward>)\n",
            "Epoch 80\n",
            "Loss tensor(0.0942, grad_fn=<MseLossBackward>)\n",
            "Epoch 90\n",
            "Loss tensor(0.0938, grad_fn=<MseLossBackward>)\n",
            "Epoch 100\n",
            "Loss tensor(0.0934, grad_fn=<MseLossBackward>)\n",
            "tensor([9.8049, 5.3857], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyM2NdKQS-AV"
      },
      "source": [
        "A very important point to note in the above code is that while updating the w and b if we write w = w - lr * (w.grad), it won't work. This is becasue this statement will store it in a new 'w'. Hence we use w -= lr * (w.grad)"
      ]
    }
  ]
}